{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In topic modeling we provide text to a (normally) unsupervised machine learning algorithm which then groups words into topics based on how they appear in the text. The most popular topic modeling algorithm is Latent-Dirichlet-Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/juliusc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/juliusc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliusc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "documents = [\n",
    "    \"I love to eat pizza. Pizza is my favorite food.\",\n",
    "    \"The cat is playing with the ball.\",\n",
    "    \"I enjoy reading books on machine learning.\",\n",
    "    \"The dog is chasing the cat.\",\n",
    "    \"Pizza and pasta are popular Italian dishes.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "Preprocessing the texts reduces the dimensionality of the problem by making everything lower case, removing common stopwords and shortening each word to its stem (lemmatize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "preprocessed_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary and corpus\n",
    "First a dictionary is created, which maps words to numbers. Then every document is transformed into numbers and stored in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(preprocessed_docs)\n",
    "\n",
    "# Convert documents into a document-term matrix.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LDA Model\n",
    "Based on the dictionary and corpus we can now start modeling the topic with LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Set parameters for LDA\n",
    "num_topics = 2  # The number of topics to find\n",
    "passes = 15     # The number of passes through the corpus during training\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Topics\n",
    "For each topic we get a collection of words that is most closely associated with this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.122*\"cat\" + 0.073*\"chasing\" + 0.073*\"dog\" + 0.073*\"ball\"\n",
      "Topic 1: 0.125*\"pizza\" + 0.073*\"eat\" + 0.073*\"favorite\" + 0.073*\"learning\"\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "for i, topic in lda_model.print_topics(num_words=4):\n",
    "    print(f\"Topic {i}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer new topics\n",
    "We can provide new text to the model to assign it to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution: [(0, 0.40136445), (1, 0.59863555)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"I love eating Italian pizza\"\n",
    "new_doc_preprocessed = preprocess(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc_preprocessed)\n",
    "\n",
    "# Infer the topic distribution for the new document\n",
    "topics = lda_model.get_document_topics(new_doc_bow)\n",
    "print(\"Topic distribution:\", topics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp4rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
